{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "id": "59Tk2W3xwboA"
      },
      "outputs": [],
      "source": [
        "text_a = open(\"1342-0.txt\").read()\n",
        "text_b = open(\"84-0.txt\").read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "scrolled": true,
        "id": "s_4qLRT-wboB",
        "outputId": "4ca56e57-4909-4e30-e053-cb4c37fd6025",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿PRIDE AND PREJUDICE\n",
            "\n",
            "By Jane Austen\n",
            "\n",
            "\n",
            "\n",
            "Chapter 1\n",
            "\n",
            "\n",
            "It is a truth universally acknowledged, that a single man in possession\n",
            "of a good fortune, must be in want of a wife.\n",
            "\n",
            "However little known the feel\n"
          ]
        }
      ],
      "source": [
        "print(text_a[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "scrolled": true,
        "id": "fZOWWjMkwboD",
        "outputId": "335d5bd7-7e07-4c0a-95d7-293dd34f4f47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['e',\n",
              " 'h',\n",
              " 'u',\n",
              " ',',\n",
              " 'l',\n",
              " 'n',\n",
              " 'n',\n",
              " 'o',\n",
              " 'i',\n",
              " 'k',\n",
              " 'k',\n",
              " 'd',\n",
              " 'r',\n",
              " 's',\n",
              " 'v',\n",
              " 'a',\n",
              " ' ',\n",
              " 'r',\n",
              " ' ',\n",
              " ' ']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import random\n",
        "random.sample(text_b, 20)  #more e's n's t's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": true,
        "id": "NBnPV6fvwboE"
      },
      "outputs": [],
      "source": [
        "a_words = text_a.split()\n",
        "b_words = text_b.split()  #split to words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": true,
        "id": "WxqLwvR4wboF",
        "outputId": "0411323d-4a35-4c8f-9007-30892e2b52c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['screen.',\n",
              " 'quite',\n",
              " 'uncommon',\n",
              " 'a',\n",
              " 'never',\n",
              " 'with',\n",
              " 'neighbour',\n",
              " 'is',\n",
              " 'impute',\n",
              " 'her.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "random.sample(a_words, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "scrolled": true,
        "id": "dSYrKDLSwboF",
        "outputId": "e104c6e4-ebca-43d9-819e-d2f77ef2b776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['weighed',\n",
              " 'secure',\n",
              " 'will',\n",
              " 'was',\n",
              " 'have',\n",
              " 'streamed',\n",
              " 'me,',\n",
              " 'looks',\n",
              " 'for',\n",
              " 'first;']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "random.sample(b_words, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "scrolled": true,
        "id": "7wShXaOGwboF",
        "outputId": "37eb8279-fdd8-49eb-a3fb-214b161f922b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' ', 111027),\n",
              " ('e', 68650),\n",
              " ('t', 45869),\n",
              " ('a', 41227),\n",
              " ('o', 39860),\n",
              " ('n', 37457),\n",
              " ('i', 35258),\n",
              " ('h', 33392),\n",
              " ('s', 32599),\n",
              " ('r', 32197),\n",
              " ('d', 21758),\n",
              " ('l', 20857)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from collections import Counter\n",
        "Counter(text_a).most_common(12) #commom letters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "scrolled": true,
        "id": "5bVkuAXIwboG",
        "outputId": "fafbd031-7cc0-4837-bb62-d17466c78086",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('to', 4051),\n",
              " ('the', 4048),\n",
              " ('of', 3556),\n",
              " ('and', 3241),\n",
              " ('a', 1889),\n",
              " ('her', 1858),\n",
              " ('was', 1794),\n",
              " ('in', 1759),\n",
              " ('I', 1740),\n",
              " ('that', 1406),\n",
              " ('not', 1337),\n",
              " ('she', 1306)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "Counter(a_words).most_common(12) #most common words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "scrolled": true,
        "id": "ZQxF53rowboH",
        "outputId": "cd934923-fc58-4f35-984d-9d6f07c10dfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 3898),\n",
              " ('and', 2903),\n",
              " ('I', 2719),\n",
              " ('of', 2636),\n",
              " ('to', 2072),\n",
              " ('my', 1631),\n",
              " ('a', 1338),\n",
              " ('in', 1071),\n",
              " ('was', 992),\n",
              " ('that', 974),\n",
              " ('had', 679),\n",
              " ('with', 654)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "Counter(b_words).most_common(12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-tR5zFkwboH"
      },
      "source": [
        "##Markov Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "scrolled": true,
        "id": "UXN3AonOwboJ",
        "outputId": "59be1a40-696e-4d18-f082-30ff8b500d79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting markovify\n",
            "  Downloading markovify-0.9.4.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode (from markovify)\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: markovify\n",
            "  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markovify: filename=markovify-0.9.4-py3-none-any.whl size=18606 sha256=cc59e282433fcb48696af34f4351e90a3a011aa7cd55b3160742b460c4c61673\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/20/eb/1a3fb93f3132f2f9683e4efd834800f80c53aeddf50e84ae80\n",
            "Successfully built markovify\n",
            "Installing collected packages: unidecode, markovify\n",
            "Successfully installed markovify-0.9.4 unidecode-1.4.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install markovify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "scrolled": true,
        "id": "pabwUO2CwboJ"
      },
      "outputs": [],
      "source": [
        "import markovify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "scrolled": true,
        "id": "at-06rNUwboK"
      },
      "outputs": [],
      "source": [
        "generator_a = markovify.Text(text_a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "scrolled": true,
        "id": "b_aYJ_oFwboK",
        "outputId": "e6a3ba4d-f671-41b8-89ad-d5cee461abe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, yes, _they_ will take care not to be assured it has taken place, for there is no end of the family obstacles which had been driven from her friend; and their conversation, which had marked their several meetings in Derbyshire; and as to render Mr. Darcy's using him ill, it is the etiquette; but no further; for on entering that place, they removed into a little confused as he had felt the necessity of his hopes, that a man really is by no means so faulty, nor Wickham's so amiable, as they returned, except what had happened at Netherfield.\n"
          ]
        }
      ],
      "source": [
        "print(generator_a.make_sentence()) #generate a sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "scrolled": true,
        "id": "FNkl2QfvwboL",
        "outputId": "60f25400-6a30-4991-c3e9-d0893b2c26e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was very little notice from any silly cause.\n"
          ]
        }
      ],
      "source": [
        "print(generator_a.make_short_sentence(50)) #short sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnqNAcecwboL"
      },
      "source": [
        "By default, Markovify tries to generate a sentence that is significantly different from any existing sentence in the input text. As a consequence, sometimes the `.make_sentence()` or `.make_short_sentence()` methods will return `None`, which means that in ten tries it wasn't able to generate such a sentence. You can work around this by increasing the number of times it tries to generate a sufficiently unique sentence using the `tries` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "scrolled": true,
        "id": "cA7-4fAPwboL",
        "outputId": "14a2ae87-aa12-488e-8e0a-8836b45643a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You will hardly hold good.\n"
          ]
        }
      ],
      "source": [
        "print(generator_a.make_short_sentence(40, tries=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGbPRyGRwboM"
      },
      "source": [
        "Or by disabling the check altogether with `test_output=False` (note that this means the generator will occasionally return stretches of text that are present in the source text):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "scrolled": true,
        "id": "KXCHnbTcwboM",
        "outputId": "0af0d223-62a7-46ab-e886-143067fd7832",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My dear Charlotte is charming.\n"
          ]
        }
      ],
      "source": [
        "print(generator_a.make_short_sentence(40, test_output=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjbMTXiNwboN"
      },
      "source": [
        "### Changing the order\n",
        "\n",
        "When you create the model, you can specify the order of the model using the `state_size` parameter. It defaults to 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "scrolled": true,
        "id": "mpu5FfkgwboN"
      },
      "outputs": [],
      "source": [
        "gen_a_1 = markovify.Text(text_a, state_size=1)\n",
        "gen_a_4 = markovify.Text(text_a, state_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "scrolled": true,
        "id": "1L2ZQZfJwboN",
        "outputId": "29cc576a-fd8c-491e-fac4-1883f388952d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "order 1\n",
            "The situation of happiness.\n",
            "\n",
            "order 4\n",
            "Their engagements at Rosings were as frequent during the last week of her stay as they had been considered in Hertfordshire.\n"
          ]
        }
      ],
      "source": [
        "print(\"order 1\")\n",
        "print(gen_a_1.make_sentence(test_output=False))\n",
        "print()\n",
        "print(\"order 4\")\n",
        "print(gen_a_4.make_sentence(test_output=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxgcxxXQwboO"
      },
      "source": [
        "In general, the higher the order, the more the sentences will seem \"coherent\" (i.e., more closely resembling the source text). Lower order models will produce more variation. Deciding on the order is usually a matter of taste and trial-and-error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2dBo3bXwboP"
      },
      "source": [
        "### Changing the level\n",
        "\n",
        "Markovify, by default, works with *words* as the individual unit. It doesn't come out-of-the-box with support for character-level models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "scrolled": true,
        "id": "5B6dMR9UwboR"
      },
      "outputs": [],
      "source": [
        "class SentencesByChar(markovify.Text):\n",
        "    def word_split(self, sentence):\n",
        "        return list(sentence)\n",
        "    def word_join(self, words):\n",
        "        return \"\".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "scrolled": true,
        "id": "j3nqi0TfwboR"
      },
      "outputs": [],
      "source": [
        "con_model = SentencesByChar(\"condescendences\", state_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "scrolled": true,
        "id": "QqI9-N0EwboS",
        "outputId": "f9677302-2eda-4985-853d-af9b99c57911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'condendencendencencescescendendendendences'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "con_model.make_sentence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ablEY_h6wboS"
      },
      "source": [
        "Of course, you can use a character-level model on any text of your choice. So, for example, the following cell creates a character-level order-7 Markov chain text generator from text A:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "scrolled": true,
        "id": "kkFlI-JbwboS"
      },
      "outputs": [],
      "source": [
        "gen_a_char = SentencesByChar(text_a, state_size=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdelfXdvwboT"
      },
      "source": [
        "And the cell below prints out a random sentence from this generator. (The `.replace()` is to get rid of any newline characters in the output.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "scrolled": false,
        "id": "c9dRHoV8wboT",
        "outputId": "74541e6f-65ef-4f86-9c1d-40bd823f544e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Young woman, and she could she think me angry, however, to obliged to know, sister all the conveyed him to Longbourn with him, I cannot always intended for allowance, and with more immediately before her person who were all but Jane had made any admirer of some of the three daughters remained and entreaty, or fortunate as I have otherwise in his coming their being yet more fortune.\n"
          ]
        }
      ],
      "source": [
        "print(gen_a_char.make_sentence(test_output=False).replace(\"\\n\", \" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_PUCZ7_wboT"
      },
      "source": [
        "### Combining models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "scrolled": true,
        "id": "YF9Ns6iBwboT"
      },
      "outputs": [],
      "source": [
        "generator_a = markovify.Text(text_a)\n",
        "generator_b = markovify.Text(text_b)\n",
        "combo = markovify.combine([generator_a, generator_b], [0.5, 0.5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b32wulqwboU"
      },
      "source": [
        "The bit of code `[0.5, 0.5]` controls the \"weights\" of the models, i.e., how much to emphasize the probabilities of any model. You can change this to suit your tastes. (E.g., if you want mostly text A with but a *soupçon* of text B, you would write `[0.9, 0.1]`. Try it!)\n",
        "\n",
        "Then you can create sentences using the combined model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "scrolled": true,
        "id": "RLyiO6ebwboU",
        "outputId": "cdc52bb3-612d-4a8e-ccb2-8b30ddc905a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The consequence of it sooner.\n"
          ]
        }
      ],
      "source": [
        "print(combo.make_sentence())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGOcBXQZwboV"
      },
      "source": [
        "### Bringing it all together\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "scrolled": true,
        "id": "fEMmUearwboV"
      },
      "outputs": [],
      "source": [
        "#Adjust where necessary\n",
        "# change to \"word\" for a word-level model\n",
        "level = \"char\"\n",
        "# controls the length of the n-gram\n",
        "order = 7\n",
        "# controls the number of lines to output\n",
        "output_n = 14\n",
        "# weights between the models; text A first, text B second.\n",
        "# if you want to completely exclude one model, set its corresponding value to 0\n",
        "weights = [0.5, 0.5]\n",
        "# limit sentence output to this number of characters\n",
        "length_limit = 280"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "scrolled": true,
        "id": "Dagc3H-mwboW",
        "outputId": "05510a6c-488f-43c4-d20b-6a78a866dae9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was along and incoherent man without any intercept the neighbourhood were got by proxy.\n",
            "\n",
            "He has been more than to plague her own easy manner too angry and tempered and agreeable in your uncle and at others, and though more amused in my new creature.\n",
            "\n",
            "My senses again, for not have on the sounds secure and thought the arch-fiend, that it was replenished birth.\n",
            "\n",
            "He walked on, and wealth rendered how excessively entered the happy; and all you give you very tall of the fire.\n",
            "\n",
            "Many of the mountains of revenge again soon, or never.\n",
            "\n",
            "His sister Gardiner, a little girl in the term, she was as well by nature as the house in the paddock.\n",
            "\n",
            "But every one wedding clothes when I awoke?\n",
            "\n",
            "I once expressing, that your being close over her into the antelope.\n",
            "\n",
            "However, that I alone could not be guilty there was little dimples appearance, but no one who superior to any other entirely reduced.\n",
            "\n",
            "He bore it would bear.\n",
            "\n",
            "But to adapt my nature, when, by the successfully away.\n",
            "\n",
            "Mr. Collins had seldom passed something port with her.\n",
            "\n",
            "Not to an air more quiet, at least knew not hope that nothing of that; they are not far from curiosity excited was very young woman another was not often met with every blast rising fondness of the card party, but my spirit.\n",
            "\n",
            "The highest opinion often walk to Rosings.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_cls = markovify.Text if level == \"word\" else SentencesByChar\n",
        "gen_a = model_cls(text_a, state_size=order)\n",
        "gen_b = model_cls(text_b, state_size=order)\n",
        "gen_combo = markovify.combine([gen_a, gen_b], weights)\n",
        "for i in range(output_n):\n",
        "    out = gen_combo.make_short_sentence(length_limit, test_output=False)\n",
        "    out = out.replace(\"\\n\", \" \")\n",
        "    print(out)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOi2KKVEwboW"
      },
      "source": [
        "## Generating with non-prose text\n",
        "\n",
        "Markovify assumes you're feeding it prose, i.e., a text file that can be parsed into sentences by separating on sentence-ending punctuation. But often you're *not* working with text like this.  \n",
        "WE USE SONNET. TXT HERE.\n",
        "We'll define the sonnet-generating task as consisting of (a) training a Markov chain on lines of poetry and then (b) generating a sequence of fourteen lines of poetry. Since the *line* is the unit now and not the *sentence*, we need to use Markovify's `NewlineText` class instead of `Text`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8nDHjZy_wboX"
      },
      "outputs": [],
      "source": [
        "sonnets_text = open(\"sonnets.txt\").read()\n",
        "sonnets_model = markovify.NewlineText(sonnets_text, state_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiEG8KK0wboX"
      },
      "source": [
        "And then we can generate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "jcqvHdd5wboX",
        "outputId": "ef40a350-e64f-4ff6-e9f0-335f09fc40fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Tempteth my heart and is reckon'd none:\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "sonnets_model.make_sentence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz53EerewboX"
      },
      "source": [
        "And now make a sonnet, sorta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "6iFn7wTqwboY",
        "outputId": "8e341ae4-d2d0-4d27-c5ab-fa78fd9fe3da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Even that you have no longer nurseth the darling buds discloses:\n",
            "And with ease we it is as is to stay your sight,\n",
            "The forward violet thus is bent my love as mine eye loves but live young.\n",
            "No; let me than tongue,\n",
            "To set me worthy of a house fall to be:\n",
            "She carv'd thee that I ey'd,\n",
            "O! let him there;\n",
            "And frantic-mad with tanned antiquity,\n",
            "That you read this shadow shadows like a second head;\n",
            "No, it hath in the eye is not me first begin.\n",
            "And suit thy husbandry?\n",
            "And shalt wane, so bright in the wise world should achieve,\n",
            "Which many now transferr'd.\n",
            "For nothing worth.\n"
          ]
        }
      ],
      "source": [
        "for i in range(14):\n",
        "    print(sonnets_model.make_sentence())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "KtglybgrwboY"
      },
      "outputs": [],
      "source": [
        "#CHARACTER BY CHARACTER NOT WORD BY WORD\n",
        "\n",
        "class LinesByChar(markovify.NewlineText):\n",
        "    def word_split(self, sentence):\n",
        "        return list(sentence)\n",
        "    def word_join(self, words):\n",
        "        return \"\".join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0G-hXUwwboZ"
      },
      "source": [
        "Now we can create a character model with the sonnets, line by line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "4Xa3bPg0wboZ"
      },
      "outputs": [],
      "source": [
        "sonnets_char_model = LinesByChar(sonnets_text, state_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yon0D9R_wboZ"
      },
      "source": [
        "And generate new sonnets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "oF7xJBzMwboZ",
        "outputId": "9e88b397-ab93-4f71-d5d6-d7b05b2edf95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As, to bright,\n",
            "When better trease the far mend.\n",
            "More weak of your heart barren thy charge with of sing on theirs forward in him brief,\n",
            "When I sense;\n",
            "Thus is my trave no make my titled guides the worms thoughts, or now what thou gave shall best then of thy do plead not thy steal sap, at have should by his my come increasure the give,\n",
            "But were long still, or hear,\n",
            "I sufferance my perjur'd by.\n",
            "And your more dying old Time that stol'n front days most that best doth fears, and suit throw.\n",
            "This sides made be nor my duty, beauty's eye\n",
            "The bears number loves me,\n",
            "For as pilgrimages refigurest\n",
            "Dear eye's place is abuse.\n",
            "So am with a for that in lovely an even their own look both love err I this pretty when dimm'd:\n",
            "I can I fear against not do should your with side;\n"
          ]
        }
      ],
      "source": [
        "for i in range(14):\n",
        "    print(sonnets_char_model.make_sentence())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naMZkzBXwboa"
      },
      "source": [
        "### New moods\n",
        "\n",
        "Character-level Markov chains are especially suitable, in my experience, for generating shorter texts, like individual words or names. Let's generate names of new moods using this technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQejc3uqwboa"
      },
      "source": [
        "Load the JSON file and grab just the list of words naming moods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "scrolled": true,
        "id": "xlJRLIh0wboa"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "mood_data = json.loads(open(\"./moods.json\").read())\n",
        "moods = mood_data['moods']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiu3hWhpwboa"
      },
      "source": [
        "The easiest way to use this is to make one big string with the moods joined together with newlines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "d0-GYCBRwbob"
      },
      "outputs": [],
      "source": [
        "moods_text = \"\\n\".join(moods)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W8Yd3FOwbob"
      },
      "source": [
        "Then use `LinesByChar` to make the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "O503GRCnwbob"
      },
      "outputs": [],
      "source": [
        "moods_char_model = LinesByChar(moods_text, state_size=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vmRcz8swbob"
      },
      "source": [
        " new moods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "HMd1omBtwbob",
        "outputId": "4cfc5bfb-3d6a-431e-cb63-f7bb393404bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stunappretchy\n",
            "troud\n",
            "victive\n",
            "sassione\n",
            "welcomforthy\n",
            "witterrogative\n",
            "unfrigid\n",
            "darise\n",
            "self-descritalgical\n",
            "suspitated\n",
            "attrappreshed\n",
            "desistified\n",
            "feistic\n",
            "overwhelpful\n",
            "peted\n",
            "tortain\n",
            "testeful\n",
            "chalaralyzed\n",
            "deted\n",
            "superky\n",
            "enricked\n",
            "conterious\n",
            "hearnishearnish\n",
            "unbalancholess\n"
          ]
        }
      ],
      "source": [
        "for i in range(24):\n",
        "    print(moods_char_model.make_sentence())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}